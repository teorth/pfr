import PFR.Mathlib.Analysis.SpecialFunctions.NegMulLog
import PFR.ForMathlib.FiniteRange
import Mathlib.Probability.IdentDistrib

/-!
# Kullback-Leibler divergence

Definition of Kullback-Leibler divergence and basic facts

## Main definitions:


## Main results

-/

open MeasureTheory ProbabilityTheory Real

variable {Œ© Œ©' Œ©'' Œ©''' G: Type*}
  [mŒ© : MeasurableSpace Œ©] {Œº : Measure Œ©}
  [mŒ©' : MeasurableSpace Œ©'] {Œº' : Measure Œ©'}
  [mŒ©'' : MeasurableSpace Œ©''] {Œº'' : Measure Œ©''}
  [mŒ©''' : MeasurableSpace Œ©'''] {Œº''' : Measure Œ©'''}
  [hG : MeasurableSpace G] [MeasurableSingletonClass G]

variable {X : Œ© ‚Üí G} {Y : Œ©' ‚Üí G} [FiniteRange X] [FiniteRange Y]

-- one should add somewhere the hypotheses that Œº, Œº', and Œº'' are all probability measures

/--  If `X, Y` are two `G`-valued random variables, the Kullback--Leibler divergence is defined as
  `KL(X ‚Äñ Y) := ‚àë‚Çì ùêè(X = x) log(ùêè(X = x) / ùêè(Y = x))`. -/
noncomputable def KL_div (X : Œ© ‚Üí G) (Y: Œ©' ‚Üí G) (Œº: Measure Œ©) (Œº' : Measure Œ©') : ‚Ñù := ‚àë' x, (Œº.map X {x}).toReal * log ((Œº.map X {x}).toReal / (Œº'.map Y {x}).toReal)

@[inherit_doc KL_div] notation3:max "KL[" X " ; " Œº " # " Y " ; " Œº' "]" => KL_div X Y Œº Œº'

/-- If `X'` is a copy of `X`, and `Y'` is a copy of `Y`, then `KL(X' ‚Äñ Y') = KL(X ‚Äñ Y)`. -/
lemma KL_div_eq_of_equiv (X' : Œ©'' ‚Üí G) (Y' : Œ©''' ‚Üí G) (hX : IdentDistrib X X' Œº Œº'') (hY : IdentDistrib Y Y' Œº' Œº''') :
  KL[X ; Œº # Y ; Œº'] = KL[X' ; Œº'' # Y' ; Œº'''] := sorry

/-- `KL(X ‚Äñ Y) ‚â• 0`.-/
lemma KL_div_nonneg : KL[X ; Œº # Y ; Œº'] ‚â• 0 := sorry

/-- `KL(X ‚Äñ Y) = 0` if and only if `Y` is a copy of `X`. -/
lemma KL_div_eq_zero_iff_identDistrib : KL[X ; Œº # Y ; Œº'] = 0 ‚Üî IdentDistrib X Y Œº Œº' := sorry

open BigOperators
/-- If $S$ is a finite set, $\sum_{s \in S} w_s = 1$ for some non-negative $w_s$, and ${\bf P}(X=x) = \sum_{s\in S} w_s  {\bf P}(X_s=x)$, ${\bf P}(Y=x) = \sum_{s\in S} w_s  {\bf P}(Y_s=x)$ for all $x$, then
$$D_{KL}(X\Vert Y) \le \sum_{s\in S} w_s D_{KL}(X_s\Vert Y_s).$$ -/
lemma KL_div_of_convex {I : Type*} {S : Finset I} {w : I ‚Üí ‚Ñù} (hw: ‚àÄ s, w s ‚â• 0) (hsum: ‚àë s in S, w s = 1) (X': I ‚Üí Œ©'' ‚Üí G) (hconvex: ‚àÄ x : G, (Œº.map X {x}).toReal = ‚àë s in S, (w s) * (Œº''.map (X' s) {x}).toReal) : KL[X ; Œº # Y ; Œº'] ‚â§ ‚àë s in S, w s * KL[X' s ; Œº'' # Y ; Œº'] := sorry

/-- If $f:G \to H$ is an injection, then $D_{KL}(f(X)\Vert f(Y)) = D_{KL}(X\Vert Y)$. -/
lemma KL_div_of_comp_inj {H : Type*} [MeasurableSpace H] [MeasurableSingletonClass H] {f : G ‚Üí H} (hf : Function.Injective f) : KL[X ; Œº # Y ; Œº'] = KL[f ‚àò X ; Œº # f ‚àò Y ; Œº'] := sorry

/-- If $X, Y, Z$ are independent $G$-valued random variables, then
  $$D_{KL}(X+Z\Vert Y+Z) \leq D_{KL}(X\Vert Y).$$ -/
lemma KL_div_add_le_KL_div_of_indep [AddCommGroup G] [MeasurableSub‚ÇÇ G] [MeasurableAdd‚ÇÇ G] {X Y Z : Œ© ‚Üí G} [FiniteRange X] [FiniteRange Y] [FiniteRange Z] (hindep : iIndepFun (fun _ ‚Ü¶ hG) ![X, Y, Z] Œº) : KL[X + Z ; Œº # Y + Z ; Œº] ‚â§ KL[X ; Œº # Y ; Œº] := sorry
