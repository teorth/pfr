\chapter{Shannon entropy inequalities}

Random variables in this paper are measurable maps $X : \Omega \to S$ from a probability space $\Omega$ to a measurable space $S$, and called $S$-valued random variables. In many cases we will assume that singletons in $S$ are measurable.  Often we will restrict further to the case when $S$ is finite with the discrete $\sigma$-algebra, which of course implies that $S$ has measurable singletons.

\begin{definition}[Entropy]
  \label{entropy-def}
  \lean{ProbabilityTheory.entropy}
  \leanok
  If $X$ is an $S$-valued random variable, the entropy $\bbH[X]$ of $X$ is defined
  $$ \bbH[X] := \sum_{s \in S} \bbP[X=x] \log \frac{1}{\bbP[X=x]}$$
  with the convention that $0 \log \frac{1}{0} = 0$.
\end{definition}

\begin{lemma}[Entropy and relabeling]\label{relabeled-entropy}\lean{ProbabilityTheory.entropy_comp_of_injective, entropy_of_comp_eq_of_comp} \uses{entropy-def}\leanok
  \begin{itemize}
\item[(i)]   If $X: \Omega \to S$ and $Y: \Omega \to T$ are random variables, and $Y = f(X)$ for some injection $f: S \to T$, then $\bbH[X] = \bbH[Y]$.
\item[(ii)]   If $X: \Omega \to S$ and $Y: \Omega \to T$ are random variables, and $Y = f(X)$ and $X = g(Y)$ for some functions $f: S \to T$, $g: T \to S$, then $\bbH[X] = \bbH[Y]$.
\end{itemize}
\end{lemma}

\begin{proof} \leanok Expand out both entropies and rearrange.
\end{proof}

\begin{lemma}[Jensen bound]\label{jensen-bound}
  \uses{entropy-def}
  \lean{ProbabilityTheory.entropy_le_log_card}
  \leanok
  If $X$ is an $S$-valued random variable, then $\bbH[X] \leq \log |S|$.
\end{lemma}

\begin{proof}\uses{jensen}\leanok
  This is a direct consequence of Lemma \ref{jensen}.
\end{proof}

\begin{definition}[Uniform distribution]\label{uniform-def}\leanok
  \lean{ProbabilityTheory.IsUniform} If $H$ is a subset of $S$, an $S$-random variable $X$ is said to be uniformly distributed on $H$ if $\bbP[X = s] = 1/|H|$ for $s \in X$ and $\bbP[X=s] = 0$ otherwise.
\end{definition}

\begin{lemma}[Uniform distributions exist]\label{unif-exist}
  \uses{uniform-def}
  \lean{ProbabilityTheory.exists_isUniform, ProbabilityTheory.exists_isUniform_measureSpace}\leanok
  Given a finite non-empty subset $H$ of a set $S$, there exists a random variable $X$ (on some probability space) that is uniformly distributed on $H$.
\end{lemma}

\begin{proof}\leanok Direct construction.
\end{proof}


\begin{lemma}[Entropy of uniform random variable]\label{uniform-entropy}
  \uses{entropy-def}
  \lean{ProbabilityTheory.IsUniform.entropy_eq}\leanok
  If $X$ is $S$-valued random variable, then $\bbH[X] = \log |S|$ if and only if $X$ is uniformly distributed on $S$.
\end{lemma}

\begin{proof} \uses{converse-jensen}\leanok Direct computation in one direction.  Converse direction needs Lemma \ref{converse-jensen}.
\end{proof}

\begin{lemma}[Entropy of uniform random variable, II]\label{uniform-entropy-II}
  \uses{entropy-def, uniform-def}
  \lean{ProbabilityTheory.IsUniform.entropy_eq}\leanok
  If $X$ is uniformly distributed on $H$, then, then $\bbH[X] = \log |H|$.
\end{lemma}

\begin{proof} Direct computation.
\end{proof}

\begin{lemma}[Bounded entropy implies concentration]\label{bound-conc}
  \uses{entropy-def}
  \lean{ProbabilityTheory.prob_ge_exp_neg_entropy}\leanok
  If $X$ is an $S$-valued random variable, then there exists $s \in S$ such that $\bbP[X=s] \geq \exp(-\bbH[X])$.
\end{lemma}

\begin{proof}
  We have
  $$ \bbH[X] = \sum_{s \in S} \bbP[X=s] \log \frac{1}{\bbP[X=s]} \geq \min_{s \in S} \log \frac{1}{\bbP[X=s]}$$
  and the claim follows.
\end{proof}

We use $X,Y$ to denote the pair $\omega \mapsto (X(\omega),Y(\omega)$).

\begin{lemma}[Commutativity and associativity of joint entropy]
  \label{entropy-comm}
  \lean{ProbabilityTheory.entropy_comm, ProbabilityTheory.entropy_assoc}
  \leanok
  If $X: \Omega \to S$, $Y: \Omega \to T$, and $Z: \Omega \to U$ are random variables, then $\bbH[X, Y] = \bbH[Y, X]$ and $\bbH[X, (Y,Z)] = \bbH[(X,Y), Z]$.
\end{lemma}
\begin{proof}
  \leanok
  \uses{relabeled-entropy}
  Set up an injection from $(X,Y)$ to $(Y,X)$ and use Lemma \ref{relabeled-entropy} for the first claim. Similarly for the second claim.
\end{proof}


\begin{definition}[Conditioned event]
  \label{condition-event-def}
  \lean{ProbabilityTheory.cond}
  \leanok
  If $X: \Omega \to S$ is an $S$-valued random variable and $E$ is an event in $\Omega$, then the conditioned event $(X|E)$ is defined to be the same random variable as $X$, but now the ambient probability measure has been conditioned to $E$.
\end{definition}

Note: it may happen that $E$ has zero measure.  In which case, the ambient probability measure should be replaced with a zero measure.  (In our formalization we achieve this by working with arbitrary measures, and normalizing them to be probability measures if possible, else using the zero measure.  Conditioning is also formalized using existing Mathlib definitions.)

\begin{definition}[Conditional entropy]
  \label{conditional-entropy-def}
  \uses{entropy-def,condition-event-def}
  \lean{ProbabilityTheory.condEntropy}
  \leanok
  If $X: \Omega \to S$ and $Y: \Omega \to T$ are random variables, the conditional entropy $\bbH[X|Y]$ is defined as
  $$ \bbH[X|Y] := \sum_{y \in Y} \bbP[Y = y] \bbH[(X | Y=y)].$$
\end{definition}

\begin{lemma}[Conditional entropy and relabeling]\label{relabeled-entropy-cond}
  \uses{conditional-entropy-def}
  \lean{ProbabilityTheory.condEntropy_comp_of_injective, ProbabilityTheory.condEntropy_of_inj_map'}\leanok
  If $X: \Omega \to S$, $Y: \Omega \to T$, and $Z: \Omega \to U$ are random variables, and $Y = f(X,Z)$ almost surely for some map $f: S \times U \to T$ that is injective for each fixed $U$, then $\bbH[X|Z] = \bbH[Y|Z]$.

  Similarly, if $g: T \to U$ is injective, then $\bbH[X|g(Y)] = \bbH[X|Y]$.
\end{lemma}

\begin{proof}
  \uses{relabeled-entropy}\leanok
  For the first part, use Definition \ref{conditional-entropy-def} and then Lemma \ref{relabeled-entropy}.  The second part is a direct computation.
\end{proof}


\begin{lemma}[Chain rule]\label{chain-rule}
  \uses{conditional-entropy-def}
  \lean{ProbabilityTheory.chain_rule, ProbabilityTheory.chain_rule'}
  \leanok
  If $X: \Omega \to S$ and $Y: \Omega \to T$ are random variables, then
  $$ \bbH[X, Y] = \bbH[Y] + \bbH[X|Y].$$
  \end{lemma}
  \begin{proof}
  \leanok
  Direct computation.
\end{proof}

\begin{lemma}[Conditional chain rule]\label{conditional-chain-rule}
  \lean{ProbabilityTheory.cond_chain_rule, ProbabilityTheory.cond_chain_rule'}\leanok
  If $X: \Omega \to S$, $Y: \Omega \to T$, $Z: \Omega \to U$ are random variables, then
$$ \bbH[X, Y | Z ] = \bbH[Y | Z] + \bbH[X|Y, Z].$$
\end{lemma}

\begin{proof}  \uses{chain-rule}\leanok
   For each $z \in U$, we can apply Lemma \ref{chain-rule} to the random variables $(X|Z=z)$ and $(Y|Z=z)$ to obtain
  $$ \bbH[ (X|Z=z),(Y|Z=z) ] = \bbH[Y|Z=z] + \bbH[(X|Z=z)|(Y|Z=z)].$$
  Now multiply by $\bbP[Z=z]$ and sum.  Some helper lemmas may be needed to get to the form above.  This sort of ``average over conditioning'' argument to get conditional entropy inequalities from unconditional ones is commonly used in this paper.
\end{proof}

\begin{definition}[Mutual information]
  \label{information-def}
  \uses{entropy-def}
  \lean{ProbabilityTheory.mutualInformation, ProbabilityTheory.mutualInformation_def}
  \leanok
  If $X: \Omega \to S$, $Y: \Omega \to T$ are random variables, then
  $$\bbI[X : Y] := \bbH[X] + \bbH[Y] - \bbH[X,Y].$$
\end{definition}

\begin{lemma}[Alternative formulae for mutual information]
  \label{alternative-mutual}
  \uses{information-def}
  \lean{ProbabilityTheory.mutualInformation_comm, ProbabilityTheory.mutualInformation_eq_entropy_sub_condEntropy}
  \leanok
  With notation as above, we have
  $$  \bbI[X : Y] = \bbI[Y:X]$$
  $$  \bbI[X : Y] = \bbH[X] - \bbH[X|Y]$$
  $$  \bbI[X : Y] = \bbH[Y] - \bbH[Y|X]$$
\end{lemma}

\begin{proof}
  \uses{entropy-comm,chain-rule}
  \leanok
  Immediate from Lemmas \ref{entropy-comm}, \ref{chain-rule}.
\end{proof}

\begin{lemma}[Nonnegativity of mutual information]
  \label{mutual-nonneg}
  \lean{ProbabilityTheory.mutualInformation_nonneg}
  \leanok
  We have $\bbI[X:Y] \geq 0$.
\end{lemma}

\begin{proof}  \uses{jensen, alternative-mutual}\leanok An application of Lemma \ref{jensen} and Lemma \ref{alternative-mutual}.
\end{proof}

\begin{corollary}[Subadditivity]
  \label{subadditive}
  \lean{ProbabilityTheory.entropy_pair_le_add}
  \leanok
  With notation as above, we have $\bbH[X,Y] \leq \bbH[X] + \bbH[Y]$.
\end{corollary}

\begin{proof}
  \uses{mutual-nonneg, alternative-mutual}
  \leanok
  Use Lemma \ref{mutual-nonneg}.
\end{proof}

\begin{corollary}[Conditioning reduces entropy]
  \label{cond-reduce}
  \lean{ProbabilityTheory.condEntropy_le_entropy}
  \leanok
  With notation as above, we have $\bbH[X|Y] \leq \bbH[X]$.
\end{corollary}
\begin{proof}
  \uses{mutual-nonneg, alternative-mutual}
  \leanok
  Combine Lemma \ref{mutual-nonneg} with Lemma \ref{alternative-mutual}.
\end{proof}

\begin{corollary}[Submodularity]\label{submodularity}
  \lean{ProbabilityTheory.entropy_submodular}\leanok
  With three random variables $X,Y,Z$, one has $\bbH[X|Y,Z] \leq \bbH[X|Z]$.
\end{corollary}

\begin{proof} \uses{cond-reduce}\leanok Apply the ``averaging over conditioning'' argument to Corollary \ref{cond-reduce}.
\end{proof}

\begin{corollary}[Alternate form of submodularity]\label{alt-submodularity}
  \lean{ProbabilityTheory.entropy_triple_add_entropy_le}\leanok
  With three random variables $X,Y,Z$, one has
  $$ \bbH[X,Y,Z] + \bbH[Z] \leq \bbH[X,Z] + \bbH[Y,Z].$$
\end{corollary}

\begin{proof}  \uses{submodularity,chain-rule} \leanok Apply Corollary \ref{submodularity} and Lemma \ref{chain-rule}.
\end{proof}

\begin{definition}[Independent random variables]
  \label{independent-def}
  \lean{ProbabilityTheory.IndepFun}
  \leanok
  Two random variables $X: \Omega \to S$ and $Y: \Omega \to T$ are independent if the law of $(X,Y)$ is the product of the law of $X$ and the law of $Y$.  Similarly for more than two variables.
\end{definition}

\begin{lemma}[Vanishing of mutual information]
  \label{vanish-entropy}
  \uses{information-def, independent-def}
  \lean{ProbabilityTheory.mutualInformation_eq_zero}
  \leanok
  If $X,Y$ are random variables, then $\bbI[X:Y] = 0$ if and only if $X,Y$ are independent.
  \end{lemma}

  \begin{proof} \uses{converse-jensen}\leanok
    An application of the equality case of Jensen's inequality, Lemma \ref{converse-jensen}.
  \end{proof}

\begin{corollary}[Additivity of entropy]\label{add-entropy}
  \lean{ProbabilityTheory.entropy_pair_eq_add}
  \leanok
  If $X,Y$ are random variables, then $\bbH[X,Y] = \bbH[X] + \bbH[Y]$ if and only if $X,Y$ are independent.
\end{corollary}

\begin{proof} \uses{vanish-entropy}\leanok
  Direct from Lemma \ref{vanish-entropy}.
\end{proof}

\begin{definition}[Conditional mutual information]
\label{conditional-mutual-def}
\uses{information-def,condition-event-def}
\lean{ProbabilityTheory.condMutualInformation}
\leanok
If $X,Y,Z$ are random variables, with $Z$ $U$-valued, then
$$ \bbI[X:Y|Z] := \sum_{z \in U} P[Z=z] \bbI[(X|Z=z): (Y|Z=z)].$$
\end{definition}

\begin{lemma}[Alternate formula for conditional mutual information]
  \label{conditional-mutual-alt}
  \uses{conditional-mutual-def}
  \lean{ProbabilityTheory.condMutualInformation_eq, ProbabilityTheory.condMutualInformation_eq'}\leanok
We have
  $$ \bbI[X:Y|Z] := \bbH[X|Z] + \bbH[Y|Z] - \bbH[X,Y|Z]$$
and
  $$ \bbI[X:Y|Z] := \bbH[X|Z] - \bbH[X|Y,Z].$$
\end{lemma}

\begin{proof}\leanok Routine computation.
\end{proof}

\begin{lemma}[Nonnegativity of conditional mutual information]
\label{conditional-nonneg}
\uses{conditional-mutual-def}
\lean{ProbabilityTheory.condMutualInformation_nonneg}\leanok
If $X,Y,Z$ are random variables, then $\bbI[X:Y|Z] \ge 0$.
\end{lemma}
\begin{proof}
\uses{submodularity}
\leanok
Use Definition \ref{conditional-mutual-def} and Lemma \ref{submodularity}.
\end{proof}

\begin{definition}[Conditionally independent random variables]
\label{conditional-independent-def}
\lean{ProbabilityTheory.condIndepFun}\leanok
  Two random variables $X: \Omega \to S$ and $Y: \Omega \to T$ are conditionally independent relative to another random variable $Z: \Omega \to U$ if $P[ X = s \wedge Y = t| Z=u] = P[X=s|Z=u] P[Y=t|Z=u]$ for all $s \in S, t \in T, u \in U$.  (We won't need conditional independence for more variables than this.)
\end{definition}

\begin{lemma}[Vanishing conditional mutual information]\label{conditional-vanish}
  \lean{ProbabilityTheory.condMutualInformation_eq_zero}
  \leanok
  If $X,Y,Z$ are random variables, then $\bbI[X:Y|Z] = 0$ iff $X,Y$ are conditionally independent over $Z$.
\end{lemma}

\begin{proof} \uses{vanish-entropy, conditional-independent-def}\leanok Immediate from Lemma \ref{vanish-entropy} and Definition \ref{conditional-independent-def}.
\end{proof}

\begin{corollary}[Entropy of conditionally independent variables]\label{cond-trial-ent}
  \lean{ProbabilityTheory.ent_of_cond_indep}\leanok
  If $X, Y$ are conditionally independent over $Z$, then
  $$ \bbH[X,Y,Z] =\bbH[X,Z] + \bbH[Y,Z] - \bbH[Z].$$
\end{corollary}

\begin{proof} \uses{conditional-vanish, conditional-mutual-alt}\leanok Immediate from Lemma \ref{conditional-vanish} and Lemma \ref{conditional-mutual-alt}.
\end{proof}
