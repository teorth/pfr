\chapter{Entropic Ruzsa calculus}

In this section $G$ will be a finite additive group.  (May eventually want to generalize to infinite $G$.)

\begin{lemma}[Negation preserves entropy]\label{neg-ent}
  \lean{ProbabilityTheory.entropy_neg}\leanok
  If $X$ is $G$-valued, then $\bbH[-X]=\bbH[X]$.
\end{lemma}

\begin{proof}
  \uses{relabeled-entropy}\leanok Immediate from \Cref{relabeled-entropy}.
\end{proof}

\begin{lemma}[Shearing preserves entropy]\label{shear-ent}
  \lean{ProbabilityTheory.condEntropy_add_right, ProbabilityTheory.condEntropy_add_left, ProbabilityTheory.condEntropy_sub_right, ProbabilityTheory.condEntropy_sub_left}\leanok
  If $X,Y$ are $G$-valued, then $\bbH[X \pm Y | Y]=\bbH[X|Y]$ and $\bbH[X \pm Y, Y] = \bbH[X, Y]$.
\end{lemma}

\begin{proof}
  \uses{relabeled-entropy-cond,chain-rule}\leanok Immediate from \Cref{relabeled-entropy-cond} and \Cref{chain-rule}.
\end{proof}

\begin{lemma}[Lower bound of sumset]\label{sumset-lower-gen}
  \lean{ProbabilityTheory.max_entropy_sub_mutualInfo_le_entropy_add, ProbabilityTheory.max_entropy_sub_mutualInfo_le_entropy_sub}\leanok
  If $X,Y$ are $G$-valued random variables on $\Omega$, we have
$$ \max(\bbH[X], \bbH[Y]) -  \bbI[X:Y] \leq \bbH[X \pm Y].$$
\end{lemma}

\begin{proof}\uses{cond-reduce, alternative-mutual, shear-ent, neg-ent} \leanok
    By \Cref{cond-reduce}, \ref{shear-ent}, \ref{alternative-mutual}, \ref{neg-ent} we have
$$
 \bbH[X\pm Y] \geq \bbH[X\pm Y|Y] = \bbH[X|Y]= \bbH[X] -  \bbI[X:Y]
$$
and similarly with the roles of $X,Y$ reversed, giving the claim.
\end{proof}

\begin{corollary}[Conditional lower bound on sumset]\label{sumset-lower-gen-cond}
  \uses{conditional-mutual-def}
  \lean{ProbabilityTheory.max_condEntropy_sub_condMutualInfo_le_condEntropy_mul, ProbabilityTheory.max_condEntropy_sub_condMutualInfo_le_condEntropy_div}\leanok
  If $X,Y$ are $G$-valued random variables on $\Omega$ and $Z$ is another random variable on $\Omega$ then
\[
  \max(\bbH[X|Z], \bbH[Y|Z]) -  \bbI[X:Y|Z] \leq \bbH[X\pm Y|Z],
\]
\end{corollary}

\begin{proof} \uses{sumset-lower-gen}\leanok This follows from \Cref{sumset-lower-gen} by conditioning to $Z = z$ and summing over $z$ (weighted by <| \bbP[Z=z]$).
\end{proof}

\begin{corollary}[Independent lower bound on sumset]
  \label{sumset-lower}
  \lean{ProbabilityTheory.max_entropy_le_entropy_add, ProbabilityTheory.max_entropy_le_entropy_sub}
  \leanok

  If $X,Y$ are independent $G$-valued random variables, then
  $$\max(\bbH[X], \bbH[Y]) \leq \bbH[X\pm Y]. $$
\end{corollary}
\begin{proof}
  \uses{sumset-lower-gen, vanish-entropy}
  \leanok

  Combine \Cref{sumset-lower-gen} with \Cref{vanish-entropy}.
\end{proof}

One random variable is said to be a copy of another if they have the same distribution.

\begin{lemma}[Copy preserves entropy]\label{copy-ent}
  \uses{entropy-def}
  \lean{ProbabilityTheory.IdentDistrib.entropy_congr}\leanok
  If $X'$ is a copy of $X$ then $\bbH[X'] = \bbH[X]$.
\end{lemma}

\begin{proof}\leanok Immediate from \Cref{entropy-def}.
\end{proof}

\begin{lemma}[Existence of independent copies]\label{independent-exist}
  \uses{independent-def}
  \lean{ProbabilityTheory.independent_copies, ProbabilityTheory.independent_copies', ProbabilityTheory.independent_copies_two, ProbabilityTheory.independent_copies3_nondep} \leanok
  Let $X_i : \Omega_i \to S_i$ be random variables for $i=1,\dots,k$.  Then if one gives $\prod_{i=1}^k S_i$ the product measure of the laws of $X_i$, the coordinate functions $(x_j)_{j=1}^k \mapsto x_i$ are jointly independent random variables which are copies of the $X_1,\dots,X_k$.
\end{lemma}

\begin{proof} \leanok
  Explicit computation.
\end{proof}

\begin{definition}[Ruzsa distance]\label{ruz-dist-def}
  \uses{entropy-def, independent-exist}
  \lean{rdist_def}\leanok
  Let $X,Y$ be $G$-valued random variables (not necessarily on the same sample space).  The \emph{Ruzsa distance} $d[X ;Y]$ between $X$ and $Y$ is defined to be
$$ d[X ;Y] := \bbH[X' - Y'] - \bbH[X']/2 - \bbH[Y']/2$$
where $X',Y'$ are (the canonical) independent copies of $X,Y$ from \Cref{independent-exist}.
\end{definition}


\begin{lemma}[Distance from zero]\label{dist-zero}
  \uses{ruz-dist-def}\lean{rdist_zero_eq_half_ent}\leanok
If $X$ is a $G$-valued random variable and $0$ is the random variable taking the value $0$ everywhere then
\[d[X;0]=\mathbb{H}(X)/2.\]
\end{lemma}

\begin{proof}\leanok
This is an immediate consequence of the definitions and $X-0\equiv X$ and $\mathbb{H}(0)=0$.
\end{proof}

\begin{lemma}[Copy preserves Ruzsa distance]\label{ruz-copy}
  \uses{ruz-dist-def}
  \lean{ProbabilityTheory.IdentDistrib.rdist_congr}\leanok
  If $X',Y'$ are copies of $X,Y$ respectively then $d[X';Y']=d[X ;Y]$.
\end{lemma}

\begin{proof} \uses{copy-ent}\leanok Immediate from Definitions \ref{ruz-dist-def} and \Cref{copy-ent}.
\end{proof}

\begin{lemma}[Ruzsa distance in independent case]\label{ruz-indep}
  \uses{ruz-dist-def}
  \lean{ProbabilityTheory.IndepFun.rdist_eq}\leanok
  If $X,Y$ are independent $G$-random variables then
  $$ d[X ;Y] := \bbH[X - Y] - \bbH[X]/2 - \bbH[Y]/2.$$
\end{lemma}

\begin{proof} \uses{relabeled-entropy, copy-ent}\leanok Immediate from \Cref{ruz-dist-def} and Lemmas \ref{relabeled-entropy}, \ref{copy-ent}.
\end{proof}

\begin{lemma}[Distance symmetric]\label{ruzsa-symm}
  \uses{ruz-dist-def}
  \lean{rdist_symm}\leanok
  If $X,Y$ are $G$-valued random variables, then
  $$ d[X ;Y] = d[Y;X].$$
\end{lemma}

\begin{proof} \uses{neg-ent}\leanok Immediate from \Cref{neg-ent} and \Cref{ruz-dist-def}.
\end{proof}

\begin{lemma}[Distance controls entropy difference]\label{ruzsa-diff}
  \uses{ruz-dist-def}
  \lean{diff_ent_le_rdist}\leanok
  If $X,Y$ are $G$-valued random variables, then
$$|\bbH[X]-H[Y]| \leq 2 d[X ;Y].$$
\end{lemma}

\begin{proof} \uses{sumset-lower, neg-ent} \leanok Immediate from \Cref{sumset-lower} and \Cref{ruz-dist-def}, and also \Cref{neg-ent}.
\end{proof}

\begin{lemma}[Distance controls entropy growth]\label{ruzsa-growth}
  \uses{ruz-dist-def}
  \lean{diff_ent_le_rdist', diff_ent_le_rdist''}\leanok
  If $X,Y$ are independent $G$-valued random variables, then
$$  \bbH[X-Y] - \bbH[X], \bbH[X-Y] - \bbH[Y] \leq 2d[X ;Y].$$
\end{lemma}

\begin{proof} \uses{sumset-lower, neg-ent} \leanok Immediate from \Cref{sumset-lower} and \Cref{ruz-dist-def}, and also \Cref{neg-ent}.
\end{proof}

\begin{lemma}[Distance nonnegative]\label{ruzsa-nonneg}
  \lean{rdist_nonneg}\leanok
  If $X,Y$ are $G$-valued random variables, then
  $$ d[X ;Y] \geq 0.$$
\end{lemma}

\begin{proof}
  \uses{ruzsa-diff}
  \leanok
  Immediate from \Cref{ruzsa-diff}.
\end{proof}


\begin{lemma}[Projection entropy and distance]\label{dist-projection}\lean{ent_of_proj_le}\leanok
If $G$ is an additive group and $X$ is a $G$-valued random variable and $H\leq G$ is a finite subgroup then, with $\pi:G\to G/H$ the natural homomorphism we have (where $U_H$ is uniform on $H$)
\[\mathbb{H}(\pi(X))\leq 2d[X;U_H].\]
\end{lemma}
\begin{proof}
\uses{independent-exist, ruzsa-diff, chain-rule, shear-ent, submodularity, jensen-bound}\leanok
WLOG, we make $X$, $U_H$ independent (\Cref{independent-exist}).
Now by Lemmas \ref{submodularity}, \ref{shear-ent}, \ref{jensen-bound}
\begin{align*}
&\mathbb{H}(X-U_H|\pi(X)) \geq \mathbb{H}(X-U_H|X) &= \mathbb{H}(U_H) \\
&\mathbb{H}(X-U_H|\pi(X)) \leq \log |H| &= \mathbb{H}(U_H)
\end{align*}
By \Cref{chain-rule}
\[\mathbb{H}(X-U_H)=\mathbb{H}(\pi(X))+\mathbb{H}(X-U_H|\pi(X))=\mathbb{H}(\pi(X))+\mathbb{H}(U_H)\]
and therefore
\[d[X;U_H]=\mathbb{H}(\pi(X))+\frac{\mathbb{H}(U_H)-\mathbb{H}(X)}{2}.\]
Furthermore by \Cref{ruzsa-diff}
\[d[X;U_H]\geq \frac{\lvert \mathbb{H}(X)-\mathbb{H}(U_H)\rvert}{2}.\]
Adding these inequalities gives the result.
\end{proof}

\begin{lemma}[Improved Ruzsa triangle inequality]
  \label{ruzsa-triangle-improved}
  \lean{ent_of_diff_le}
  \leanok
  If $X,Y,Z$ are $G$-valued random variables on $\Omega$ with $(X,Y)$ independent of $Z$, then
  \begin{equation}\label{submod-explicit} \bbH[X - Y] \leq \bbH[X-Z] + \bbH[Z-Y] - \bbH[Z]\end{equation}
\end{lemma}

This is an improvement over the usual Ruzsa triangle inequality because $X,Y$ are not assumed to be independent.  However we will not utilize this improvement here.

\begin{proof}
  \uses{alt-submodularity,subadditive, relabeled-entropy, add-entropy}
  \leanok
  Apply \Cref{alt-submodularity} to obtain
  \[\bbH[X - Z, X - Y] + \bbH[Y, X - Y] \geq \bbH[X - Z, Y, X - Y] + \bbH[X - Y].\]
  Using
  \[\bbH[X - Z, X - Y] \leq \bbH[X - Z] + \bbH[Y - Z]\]
  (from \Cref{relabeled-entropy}, \Cref{subadditive}),
  \[\bbH[Y, X - Y] = \bbH[X, Y] \]
  (from \Cref{relabeled-entropy}), and
  \[\bbH[X - Z, Y, X - Y] = \bbH[X, Y, Z] = \bbH[X, Y] + \bbH[Z]\]
  (from \Cref{relabeled-entropy} and \Cref{add-entropy}) and rearranging, we indeed obtain~\eqref{submod-explicit}.
\end{proof}

\begin{lemma}[Ruzsa triangle inequality]
  \label{ruzsa-triangle}
  \lean{rdist_triangle}
  \leanok
  If $X,Y,Z$ are $G$-valued random variables, then
$$ d[X ;Y] \leq d[X ;Z] + d[Z;Y].$$
\end{lemma}

\begin{proof}\uses{ruz-copy, ruz-indep, independent-exist, ruzsa-triangle-improved}\leanok By \Cref{ruz-copy} and Lemmas \ref{independent-exist}, \ref{ruz-indep}, it suffices to prove this inequality assuming that $X,Y,Z$ are defined on the same space and are independent.  But then the claim follows from \Cref{ruzsa-triangle-improved} and \Cref{ruz-dist-def}.
\end{proof}

\begin{definition}[Conditioned Ruzsa distance]\label{cond-dist-def}
  \uses{ruz-dist-def}
  \lean{condRuzsaDist}\leanok
If $(X, Z)$ and $(Y, W)$ are random variables (where $X$ and $Y$ are $G$-valued) we define
$$ d[X  | Z; Y | W] := \sum_{z,w}  \bbP[Z=z]  \bbP[W=w] d[(X|Z=z); (Y|(W=w))].$$
similarly
$$ d[X ; Y | W] := \sum_{w}  \bbP[W=w] d[X ; (Y|(W=w))].$$
\end{definition}

\begin{lemma}[Alternate form of distance]\label{cond-dist-alt}
\lean{condRuzsaDist_of_copy, condRuzsaDist'_of_copy, condRuzsaDist_of_indep, condRuzsaDist'_of_indep}\leanok
  The expression $d[X  | Z;Y | W]$ is unchanged if $(X,Z)$ or $(Y,W)$ is replaced by a copy.  Furthermore, if $(X,Z)$ and $(Y,W)$ are independent, then
$$  d[X  | Z;Y | W] = \bbH[X-Y|Z,W] - \bbH[X|Z]/2 - \bbH[Y|W]/2$$
and similarly
$$  d[X ;Y | W] = \bbH[X-Y|W] - \bbH[X]/2 - \bbH[Y|W]/2.$$
\end{lemma}

\begin{proof}\uses{copy-ent, ruz-copy, ruz-indep, cond-dist-def, conditional-entropy-def}\leanok Straightforward thanks to \Cref{copy-ent}, \Cref{ruz-copy}, \Cref{ruz-indep}, \Cref{cond-dist-def}, \Cref{conditional-entropy-def}.
\end{proof}

\begin{lemma}[Kaimanovich-Vershik-Madiman inequality]\label{kv}
  \lean{kaimanovich_vershik}\leanok
Suppose that $X, Y, Z$ are independent $G$-valued random variables. Then
\[
  \bbH[X + Y + Z] - \bbH[X + Y] \leq \bbH[Y+ Z] - \bbH[Y].
\]
\end{lemma}

\begin{proof}\uses{submodularity, add-entropy, relabeled-entropy}\leanok
From \Cref{submodularity} we have
$$ \bbH[X, X + Y+ Z] + \bbH[Z, X + Y+ Z] \geq \bbH[X, Z, X + Y+ Z] + \bbH[X + Y+ Z].$$
However, using Lemmas \ref{add-entropy}, \ref{relabeled-entropy} repeatedly we have $\bbH[X, X + Y+ Z] = \bbH[X, Y+ Z] = \bbH[X] + \bbH[Y+ Z]$, $\bbH[Z, X + Y + Z] = \bbH[Z, X + Y] = \bbH[Z] + \bbH[X + Y]$ and $\bbH[X, Z, X + Y+ Z] = \bbH[X, Y, Z] = \bbH[X] + \bbH[Y] + \bbH[Z]$.  The claim then follows from a calculation.
\end{proof}

\begin{lemma}[Existence of conditional independent trials]\label{cond-indep-exist}
  \uses{conditional-independent-def}
  \lean{ProbabilityTheory.condIndep_copies}\leanok
  For $X,Y$ random variables, there exist random variables $X_1,X_2,Y'$ on a common probability space with $(X_1, Y'), (X_2, Y')$ both having the distribution of $(X,Y)$, and $X_1, X_2$ conditionally independent over $Y'$ in the sense of \Cref{conditional-independent-def}.
\end{lemma}

\begin{proof}\leanok Explicit construction.
\end{proof}


\begin{lemma}[Balog-Szemer\'edi-Gowers]\label{entropic-bsg}
  \lean{ent_bsg}\leanok
  Let $A,B$ be $G$-valued random variables on $\Omega$, and set $Z := A+B$.
Then
\begin{equation}\label{2-bsg-takeaway} \sum_{z}  \bbP[Z=z] d[(A | Z = z); (B | Z = z)] \leq 3  \bbI[A:B] + 2 \bbH[Z] - \bbH[A] - \bbH[B]. \end{equation}
\end{lemma}

\begin{proof}
  \uses{cond-indep-exist, cond-trial-ent, conditional-entropy-def,submodularity, copy-ent, relabeled-entropy, add-entropy, ruz-indep}
  \leanok
Let $(A_1, B_1)$ and $(A_2, B_2)$ (and $Z'$, which by abuse of notation we call $Z$) be conditionally independent trials of $(A,B)$ relative to $Z$ as produced by \Cref{cond-indep-exist}, thus $(A_1,B_1)$ and $(A_2,B_2)$ are coupled through the random variable $A_1 + B_1 = A_2 + B_2$, which by abuse of notation we shall also call $Z$.

Observe from \Cref{ruz-indep} that the left-hand side of~\eqref{2-bsg-takeaway} is
\begin{equation}\label{lhs-to-bound}
\bbH[A_1 - B_2| Z] - \bbH[A_1 | Z]/2 - \bbH[B_2 | Z]/2.
\end{equation}
since, crucially, $(A_1 | Z=z)$ and $(B_2 | Z=z)$ are independent for all $z$.

Applying submodularity (\Cref{alt-submodularity}) gives
\begin{equation}\label{bsg-31} \begin{split}
&\bbH[A_1 - B_2] + \bbH[A_1 - B_2, A_1, B_1] \\
&\qquad \leq \bbH[A_1 - B_2, A_1] + \bbH[A_1 - B_2,B_1].
\end{split}\end{equation}
We estimate the second, third and fourth terms appearing here.
First note that, by \Cref{cond-trial-ent} and \Cref{relabeled-entropy} (noting that the tuple $(A_1 - B_2, A_1, B_1)$  determines the tuple $(A_1, A_2, B_1, B_2)$ since $A_1+B_1=A_2+B_2$)
\begin{equation}\label{bsg-24} \bbH[A_1 - B_2, A_1, B_1] = \bbH[A_1, B_1, A_2, B_2,Z] = 2\bbH[A,B] - \bbH[Z].\end{equation}
Next observe that
\begin{equation}\label{bsg-23} \bbH[A_1 - B_2, A_1] = \bbH[A_1, B_2] \leq \bbH[A] + \bbH[B].
\end{equation}
Finally, we have
\begin{equation}\label{bsg-25} \bbH[A_1 - B_2, B_1] = \bbH[A_2 - B_1, B_1] = \bbH[A_2, B_1] \leq \bbH[A] + \bbH[B].\end{equation}
Substituting~\eqref{bsg-24},~\eqref{bsg-23} and~\eqref{bsg-25} into~\eqref{bsg-31} yields
\[\bbH[A_1 - B_2] \leq 2 \bbI[A:B] + \bbH[Z]\] and so by \Cref{cond-reduce}
\[\bbH[A_1 - B_2 | Z]  \leq 2 \bbI[A:B] + \bbH[Z].\]
Since
\begin{align*} \bbH[A_1 | Z] & = \bbH[A_1, A_1 + B_1] - \bbH[Z] \\ & = \bbH[A,B] - \bbH[Z] \\ & = \bbH[Z] -  \bbI[A:B] - 2 \bbH[Z]- \bbH[A]-\bbH[B]\end{align*}
and similarly for $\bbH[B_2 | Z]$, we see that~\eqref{lhs-to-bound} is bounded by
$3 \bbI[A:B] + 2\bbH[Z]-\bbH[A]-\bbH[B]$ as claimed.
\end{proof}


\begin{lemma}[Upper bound on conditioned Ruzsa distance]\label{cond-dist-fact}
  \uses{cond-dist-def, information-def}
  \lean{condRuzsaDist_le, condRuzsaDist_le'}\leanok
  Suppose that $(X, Z)$ and $(Y, W)$ are random variables, where $X, Y$ take values in an abelian group. Then
  \[   d[X  | Z;Y | W] \leq d[X ; Y] + \tfrac{1}{2}  \bbI[X : Z] + \tfrac{1}{2}  \bbI[Y : W].\]
  In particular,
  \[   d[X ;Y | W] \leq d[X ; Y] + \tfrac{1}{2}  \bbI[Y : W].\]
\end{lemma}

\begin{proof}
\uses{cond-dist-alt, independent-exist, cond-reduce}\leanok
Using \Cref{cond-dist-alt} and \Cref{independent-exist}, if $(X',Z'), (Y',W')$ are independent copies of the variables $(X,Z)$, $(Y,W)$, we have
\begin{align*}
  d[X  | Z; Y | W]&= \bbH[X'-Y'|Z',W'] - \tfrac{1}{2} \bbH[X'|Z'] - \tfrac{1}{2}H[Y'|W'] \\
                       &\le \bbH[X'-Y']- \tfrac{1}{2} \bbH[X'|Z'] - \tfrac{1}{2}H[Y'|W'] \\
                       &= d[X';Y'] + \tfrac{1}{2}  \bbI[X' : Z'] + \tfrac{1}{2}  \bbI[Y' : W'].
\end{align*}
Here, in the middle step we used \Cref{cond-reduce}, and in the last step we used \Cref{ruz-dist-def} and \Cref{information-def}.
\end{proof}

\begin{lemma}[Comparison of Ruzsa distances, I]\label{first-useful}
  \lean{condRuzsaDist_diff_le, condRuzsaDist_diff_le', condRuzsaDist_diff_le'', condRuzsaDist_diff_le'''}\leanok
  Let $X, Y, Z$ be random variables taking values in some abelian group of characteristic $2$, and with $Y, Z$ independent. Then we have
  \begin{align}\nonumber d[X ; Y + Z] -d[X ; Y] &  \leq \tfrac{1}{2} (\bbH[Y+ Z] - \bbH[Y]) \\ & = \tfrac{1}{2} d[Y; Z] + \tfrac{1}{4} \bbH[Z] - \tfrac{1}{4} \bbH[Y]. \label{lem51-a} \end{align}
  and
  \begin{align}\nonumber
  d[X ;Y|Y+ Z] - d[X ;Y] & \leq \tfrac{1}{2} \bigl(\bbH[Y+ Z] - \bbH[Z]\bigr) \\ & = \tfrac{1}{2} d[Y;Z] + \tfrac{1}{4} \bbH[Y] - \tfrac{1}{4} \bbH[Z].
    \label{ruzsa-3}
  \end{align}
  \end{lemma}

  \begin{proof}
    \uses{ruz-copy, independent-exist, kv, ruz-indep, relabeled-entropy, cond-dist-fact}\leanok
  We first prove~\eqref{lem51-a}. We may assume (taking an independent copy, using \Cref{independent-exist} and \Cref{ruz-copy}, \ref{ruz-indep}) that $X$ is independent of $Y, Z$. Then we have
  \begin{align*}  d[X ;Y+ Z] & - d[X ;Y] \\ & = \bbH[X + Y + Z] - \bbH[X + Y] - \tfrac{1}{2}\bbH[Y + Z] + \tfrac{1}{2} \bbH[Y].\end{align*}
  Combining this with \Cref{kv} gives the required bound. The second form of the result is immediate \Cref{ruz-indep}.

  Turning to~\eqref{ruzsa-3}, we have from \Cref{information-def} and \Cref{relabeled-entropy}
  \begin{align*}  \bbI[Y : Y+ Z] & = \bbH[Y] + \bbH[Y + Z] - \bbH[Y, Y + Z] \\ & = \bbH[Y] + \bbH[Y + Z] - \bbH[Y, Z]  = \bbH[Y + Z] - \bbH[Z],\end{align*}
  and so~\eqref{ruzsa-3} is a consequence of \Cref{cond-dist-fact}. Once again the second form of the result is immediate from \Cref{ruz-indep}.
\end{proof}

\begin{lemma}[Comparison of Ruzsa distances, II]\label{second-useful}
  \lean{condRuzsaDist_diff_ofsum_le}\leanok
  Let $X, Y, Z, Z'$ be random variables taking values in some abelian group, and with $Y, Z, Z'$ independent. Then we have
  \begin{align}\nonumber
  & d[X ;Y + Z | Y + Z + Z'] - d[X ;Y] \\ & \qquad \leq \tfrac{1}{2} ( \bbH[Y + Z + Z'] + \bbH[Y + Z] - \bbH[Y] - \bbH[Z']).\label{7111}
  \end{align}
  \end{lemma}

  \begin{proof}
    \uses{first-useful}\leanok
  By \Cref{first-useful} (with a change of variables) we have
  \[d[X ; Y + Z | Y + Z + Z'] - d[X ; Y + Z] \leq \tfrac{1}{2}( \bbH[Y + Z + Z'] - \bbH[Z']).\]
  Adding this to~\eqref{lem51-a} gives the result.
  \end{proof}
